<!doctype html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The Environmental Cost Of Training A Large Language Model | Ruben Gres</title><meta name="title" content="The Environmental Cost Of Training A Large Language Model | Ruben Gres"><meta name="description" content="blog"><meta name="keywords" content="ecology,AI,LLM,eleventy,template,simple,clean"><meta name="author" content="Ruben Gres"><meta name="robots" content="index, follow"><link rel="canonical" href="https://rubengr.es/ai_environmental_cost/"><link rel="shortcut icon" type="image/png" href="/blog/assets/img/favicon.png"><link rel="apple-touch-icon" href="/blog/assets/img/apple-touch-icon.png"><link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://fonts.gstatic.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap"><link rel="stylesheet" href="/blog/assets/css/main.css"></head><body class="flex flex-col h-screen bg-white text-slate-800 break-words"><header id="header" class="bg-white w-full px-6 py-5 z-50 fixed top-0 shadow-md transition-all transform ease-in-out duration-500"><div class="max-w-5xl mx-auto flex items-center flex-wrap justify-between"><div class="sm:mr-8"><a class="flex items-center" href="http://rubengr.es"><span class="text-xl text-teal-700 font-semibold self-center">Ruben Gres</span></a></div><nav id="menu" class="order-last md:order-none items-center flex-grow w-full md:w-auto md:flex hidden mt-2 md:mt-0"><a href="https://rubengr.es/blog" class="block mt-4 md:inline-block md:mt-0 font-medium text-slate-700 hover:text-teal-600 text-base mr-4">Blog</a> <a href="https://rubengr.es/publications" class="block mt-4 md:inline-block md:mt-0 font-medium text-slate-700 hover:text-teal-600 text-base mr-4">Publications</a> <a href="https://rubengr.es/teaching" class="block mt-4 md:inline-block md:mt-0 font-medium text-slate-700 hover:text-teal-600 text-base mr-4">Teaching</a> <a href="https://rubengr.es/resume" class="block mt-4 md:inline-block md:mt-0 font-medium text-slate-700 hover:text-teal-600 text-base mr-4">Resume</a> <a href="https://rubengr.es/about" class="block mt-4 md:inline-block md:mt-0 font-medium text-slate-700 hover:text-teal-600 text-base mr-4">About</a></nav><form id="search" action="/blog/search" class="order-last sm:order-none flex-grow items-center justify-end hidden sm:block mt-6 sm:mt-0"><label class="visually-hidden" for="header-searchbox">Search here ...</label> <input type="text" id="header-searchbox" name="q" placeholder="Search here ..." class="w-full sm:max-w-xs bg-slate-200 border border-transparent float-right focus:bg-white focus:border-slate-300 focus:outline-none h-8 p-4 placeholder-slate-500 rounded text-slate-700 text-sm"></form><div id="menu-toggle" class="flex items-center md:hidden text-slate-700 hover:text-teal-600 cursor-pointer sm:ml-6"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></div></div></header><main class="mx-7 lg:mx-6 mt-32 flex-grow"><article class="max-w-5xl mx-auto"><header class="mb-14"><h1 class="text-3xl text-center font-bold leading-normal text-slate-900 mt-0 mb-3">The Environmental Cost Of Training A Large Language Model</h1><div class="text-center">Published on 26 September 2024</div><div class="mt-3 text-center"><a href="/blog/tags/ecology" class="inline-block bg-slate-200 rounded-full px-3 py-1 text-sm font-medium text-slate-700 m-0.5">#ecology</a> <a href="/blog/tags/AI" class="inline-block bg-slate-200 rounded-full px-3 py-1 text-sm font-medium text-slate-700 m-0.5">#AI</a> <a href="/blog/tags/LLM" class="inline-block bg-slate-200 rounded-full px-3 py-1 text-sm font-medium text-slate-700 m-0.5">#LLM</a></div></header><div id="content" class="prose text-slate-800 max-w-none"><p>Recently I've read the book &quot;Ralentir ou périr. L'économie de la décroissance&quot; by Thimothée Parrique, an essay on degrowth and what it could look like. In his essay, Thimothée cites a <a href="https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/">MIT 2019 study</a> that concluded that &quot;Training a single AI model can emit as much carbon as five cars in their lifetimes&quot;</p><p>Wait a minute... five cars is not <strong>that</strong> much is it?</p><p>Here is a figure from the accompanying article comparing common carbon footprints:</p><p><img src="/blog/assets/img/carbon_footprint.png" alt="xxxlarge"></p><p>At the time, the study focused on GPT-2, the Large Language Model (LLM) that eventually became ChatGPT. Five cars isn't that much, how come <a href="https://www.ibanet.org/sustainability-big-techs-ai-push-putting-climate-targets-at-risk">Microsoft increased its carbon emissions by 30 per cent since 2020 and Google its greenhouse gas emissions (GHG) by 48 per cent since 2019</a> and blames ML if the cost of training a LLM is only... five cars? Seems like the rules have changed since 2019.</p><p>Now that we're in 2024 and the world is going through an AI craze, let's see how many cars it takes to train a state of the art language model.</p><p><img src="/blog/assets/img/five_cars.png" alt="xlarge"></p><h1>Meta sells your data but shares its research</h1><p>The metaverse didn't catch on...</p><p>When Meta finally admitted it, they went all in on AI and started making models to compete against OpenAI. Their strategy is a bit different though as they prefer focusing on usecases and distribute their model in open source, even allowing commmercial use.</p><p>Anyone (or anyone with a good enough computer) can download the latest state of the art open source model, download the research paper and learn how Meta approaches building their LLM. Meta commitment to open source allow us to have some insight of what it take to train a <a href="https://en.wikipedia.org/wiki/Stochastic_parrot">stochastic <s>parrot</s></a> <a href="https://www.lesswrong.com/posts/yjzW7gxk2h7bBs2qr/the-meaning-of-shoggoth-ai-memes">soggoth</a> in the post ChatGPT era.</p><p>So let's do just that and find out how many cars it takes to train a single AI model... in 2024.</p><p><img src="/blog/assets/img/soggoth_reading.png" alt="xlarge"></p><h1>Llama 3.1 and its paper</h1><p><em>The 23rd July</em>, Meta released its new family of models in the Llama family. This new version, LLama 3.1 declines in three versions of different sizes and capabilities: <strong>8B, 70B and 405B</strong>. (&quot;B&quot; roughly represents a billion neurons: the bigger, the better)</p><p>This new family of models arrived by storm in the LLM community, and for good reasons: it's the best <em>open source</em> model to date, rivalizing with the latest version of ChatGPT at the time of release.</p><p>But contrary to ChatGPT, the release came with a <a href="https://arxiv.org/abs/2407.21783">92 page scientific paper</a> that goes into great technical details on the training, alignement and testing process. It's detailled enough so that we can start to derive some number from there.</p><h2>What we learn from the paper</h2><p>The first two families of Llama models were trained on Meta's supercluster anounced in <a href="https://ai.meta.com/blog/ai-rsc/">January 2022</a>, at the time it was comprised of <strong>6,080 A100 GPU.</strong> But this supercluster was not enough for the new Llama models that were trained on a <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">brand new 48k H100 GPU cluster</a></p><p>By the end of 2024, Meta aims to scale this supercomputer with the processing power of 600k H100 GPUs. (<em>Guess why there is a shortage of NVIDIA GPUs these days?</em>) If you were to buy all of theses GPU yourself the total cost would be around eighteen billion dollars. <em>EIGHTEEN - BILLION - DOLLARDS</em>. This represents the tenth of Meta revenues, and we're only counting the cost of the GPUs.</p><p>That sounds like it uses more than five cars, maybe we'll need a bigger garage...</p><h2>Let's crunch the numbers</h2><p>Of course all of the cluster is not used entirely at once. No more than 16k GPUs are used for training at any given time. <a href="https://www.nvidia.com/fr-fr/data-center/h100/">According to nvidia</a>, each H100 uses around <strong>700W</strong> of electricity when in use.</p><p>The training lasted around a hundred days, which for 16k GPU is coherent with the <strong>39.3M GPU</strong> hours in the paper. If we do a simple multiplication we can guess the number of Watt per hour that was poured into training Llama3.1</p><p><img src="/blog/assets/img/equation.png" alt="xlarge"></p><p><strong>27,510 MWh!</strong> If we take the <a href="https://www.solarreviews.com/blog/how-many-kwh-does-a-house-use">average US household energy consumption</a> to have an idea of scale, training theses models uses enough electricity to cover the needs of <strong>ten thousand households for the 3 months it took to train the model</strong>, thirty thousand if thoses were french households.</p><p>Meta does not disclose the location of its training datacenter, let's compare what this would cost in different countries using the <a href="https://app.electricitymaps.com/map">ElectricityMap website</a></p><table><thead><tr><th>Country</th><th>gCO₂/kWh</th><th>Llama3.1 training in tons CO₂</th></tr></thead><tbody><tr><td>Sweden</td><td>13</td><td>357</td></tr><tr><td>France</td><td>25</td><td>687</td></tr><tr><td>Germany</td><td>250</td><td>6, 875</td></tr><tr><td>United States</td><td>400-700</td><td>11, 000 - 19, 250</td></tr></tbody></table><p>Realistically, this datacenter is probably in the United States, and if Meta made the effort to chose a state were the electricity is low carbon, <strong>11k tons of CO₂ should be pretty close to reality</strong>.</p><p>Dr. Sasha Luccioni — who you should follow — posted a <a href="https://www.linkedin.com/posts/sashaluccioniphd_congrats-to-meta-on-tracking-and-releasing-activity-7221603881791688705-1Fcj">Linkedin post</a> a few days after the model release that included a table for training CO₂ emissions. She arrived at a total of <strong>11,390 tons of CO₂eq emissions</strong>, so my calculations isn't far off!</p><p>Going back to the figure for US cars in the beggining of this article, we can finally calculte how many cars it takes to train an LLM nowadays, using Sasha's estimation because I trust her more than me:</p><p><img src="/blog/assets/img/equation2.png" alt="xxlarge"></p><p>Finally, we can update the MIT Technology review article:</p><blockquote><p>Training a single AI model can emit as much carbon as a <s>five</s> <strong>one hundred eighty one</strong> cars in their lifetimes</p></blockquote><p>So since 2019, training a state of the art Large Language Model is now <strong>36 times</strong> more damaging to the planet, and accounts for a pretty big traffic jam.</p><p><img src="/blog/assets/img/trafficjam.png" alt="xxlarge"></p><h2>Hidden costs and roadblocks</h2><p>This figure is without counting the embodied CO2, i.e the cost of manufacturing the data center where this training has taken place. Nvidia does not currently disclose the carbon footprint of its GPUs but from an <a href="https://arxiv.org/pdf/2211.02001">article by Sasha Luccioni et al.</a> we estimate this cost to be about <strong>150 kg of CO2eq / GPU</strong>.</p><p>For our 16k H100 this accounts for <strong>2,400 tons more</strong>, assuming the carbon cost for building a GPU stayed about the same since this article was written (which it hasn't). Theses GPU will be used for training other models so it's not obvious how to account for them in our calculations.</p><p>This isn't even accounting for the inference cost — the cost of running the model — that is way higher than the cost of training, and very hard to quantify.</p><p>The mere amount of electricity is not the only problem they face when building large language models, let's look at this excerpt from the paper:</p><blockquote><p>During training, tens of thousands of GPUs may increase or decrease power consumption at the same time [...]. When this happens, it can result in instant fluctuations of power consumption across the data center <strong>on the order of tens of megawatts</strong>, <strong>stretching the limits of the power grid</strong>. This is an ongoing challenge for us as we scale training for future, even larger Llama models.</p></blockquote><p>A major roadblock for the next generation AI is power consumption.</p><p>Meta is not the only one facing this issue as <a href="https://fortune.com/2024/09/27/openai-5gw-data-centers-altman-power-requirements-nuclear/">OpenAI wants to build 5-gigawatt datacenters</a>. That's the output of five nuclear plants, for a single datacenter.</p><p>A solution would be to plug a dedicated power plant directly to the datacenter. We're starting to see some signs of this: Microsoft is <a href="https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/">refurbishing a nuclear power plant</a> for their datacenters and <a href="https://heatmap.news/technology/amazon-nuclear-talen-energy">Amazon is heavily investing in nuclear energy</a>.</p><p>Or, you know, just crash the grid.</p><p><img src="/blog/assets/img/llama_strip.png" alt="xxxlarge"></p><h1>So, what do we do now?</h1><p>GenAI isn't slowing down any time soon as LLMs start to appear in every product we use everyday.</p><p>Companies are in a arms race to build the biggest and most powerful model, but we still have a lot to learn from what we can do with our current ones. OpenAI recently released o1, which is not bigger but achieves better results by &quot;taking the time to think&quot; before answering. I believe approaches like this are promising in the near future to achieve better results with same sized model.</p><p>In the meantime you can try being concious about your usage. For example, avoid going straight to ChatGPT when a simple google search would have done the job. And save a little bit of energy every time.</p><p>Or go all in and hope our AI overlords can solve our current life threatening global climate crisis.</p><p><img src="/blog/assets/img/firecamp_meme.png" alt="xxlarge"></p></div></article></main><footer class="mt-20 px-10 py-8 bg-slate-200"><div class="max-w-5xl mx-auto text-slate-700 text-center">© 2023 <a href="/blog/" class="font-medium" target="_blank" rel="noopener">Ruben Gres</a>.</div></footer><script src="/blog/assets/js/bundle.js"></script></body></html>