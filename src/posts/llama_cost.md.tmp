---

title: "Watt Does It Take to Train a LLaMA model?"

date: 2024-05-15

thumb: "llama_cost.jpg"

tags:

    - LLM

    - ecology

---

  

In 2019 — only five years ago — MIT did a study of the cost of AI and arrived at the conclusion that ["Training a single AI model can emit as much carbon as five cars in their lifetimes"](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/)
  
Wait... that's not **that** much is it?

[Than how come Microsoft increased its carbon emissions by 30 per cent since 2020 and Google its greenhouse gas emissions (GHG) by 48 per cent since 2019 ?](https://www.ibanet.org/sustainability-big-techs-ai-push-putting-climate-targets-at-risk) 

The simplicity of this number stuck with me and that's what I had in mind when talking about the carbon cost of generative AI.

Now that we're in 2024 and the world is going through an AI craze, how many cars does it take to train a state of the art LLM?

# Llama 3.1 and its paper


The 23rd July, Meta released its new family of models in the Llama family. This new version, LLama 3.1 declines in three versions of different sizes and capabilities: 8B, 70B and 405B. 

This new model arrived by storm in the LLM community, and for good reasons: it's the best **open source** model to date, rivalizing with the last version of ChatGPT at the time of release.

What's super cool is the 90 page paper released along side the model that goes into great technical details on the training, alignement and testing process. It's detailled enough so that we can start to derive some number from there.

Let's take a closer look and try to figure out how many cars it takes to train Llama 3.1.

# Meta steals your data but is open about its research

The metaverse was a flop.  

When Meta finally realized this they went all in in AI and started making models to compete against OpenAI. Their strategy is a bit different though and they prefer focusing on applications and distribute their model in open source and even allow commmercial uses.

Meta commitment to open source allow us to have some insight of what it take to train this beast of a model.

//TODO soggoth reading .jpg
  

# What we learn from the paper


The first two families of Llama models were trained on Meta's supercluster anounced in [January 2022](https://ai.meta.com/blog/ai-rsc/), at the time it was comprised of 6,080 A100 GPU. But this supercluster was not enough for the new Llama models that was trained on a [brand new 48k H100 GPU cluster](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)  

By the end of 2024, Meta aims to scale this supercomputer with the processing power of 600k H100 GPUs. Can you guess why there is a shortage of NVIDIA GPUs now ?  

If you were to buy all of theses GPU yourself the total cost would be around eighteen billion dollars.   
EIGHTEEN BILLION DOLLARDS. **That's around the gross domestic product of China in 2022**

From the car analogy, maybe we'll need a bigger garage...

# Let's crunch the numbers

  
All of the cluster is not used entirely at once though, and no more than 16k GPU at a time are used for training.

On when training is resuming after synchronisation of the GPUs:

"When this happens, it can result in instant fluctuations of power

consumption across the data center on the order of tens of megawatts, stretching the limits of the power grid.

This is an ongoing challenge for us as we scale training for future, even larger Llama models."

  

This table was stolen from ???

| Model          | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) |

|----------------|---------------------------|--------------------------------|----------------------------------------------------------------|

| Llama 3.1 8B   | 1.46M                      | 700                            | 420                                                            |

| Llama 3.1 70B  | 7.0M                       | 700                            | 2,040                                                          |

| Llama 3.1 405B | 30.84M                     | 700                            | 8,930                                                          |

| **Total**      | **39.3M**                  | **700**                        | **11,390**                                                     |

  

This is without counting the embodied CO2, i.e the cost of manufacturing the data center where this trainin has taken place.

Let's try to take this into account.

  

[150 kg of CO2eq / GPU ](https://arxiv.org/pdf/2211.02001)

  

TODO

  

# Failures of GPUs

  

# Inference costs

We did not even start accounting for inference costs

# So, what do we do now?

  

Open Sourcing models to avoid retraining ?

Limiting inference time by not going straight to an LLM

Use smaller models when possible

  

# Future perspectives

  

Are data center getting better ?

AI Gen getting more and more popular

Big tech is focused on building a bigger and better model, and there is no going back

Do we even need it ? => We have yet to explore what we can do with our current models

Every company trains it own model, and most are murican

10 google research = 1 chatgpt call

  

# Sources

https://nathanbaileyw.medium.com/the-carbon-footprint-of-llms-a-disaster-in-waiting-6fc666235cd0

https://www.usinenouvelle.com/article/le-succes-de-l-intelligence-artificielle-fait-bondir-l-empreinte-carbone-des-acteurs-du-secteur.N2213817

https://promptengineering.org/llm-open-source-vs-open-weights-vs-restricted-weights/

https://medium.com/gptalk/is-llama-3-1-really-open-source-73ac220f9aa2
