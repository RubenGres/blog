---
title: "Watt Does It Take to Train a LLaMA model?"
date: 2024-05-15
thumb: "llama_cost.jpg"
tags:
    - LLM
    - ecology
---

In 2019 — only five years ago — MIT did a study of the cost of AI and arrived at the conclusion that ["Training a single AI model can emit as much carbon as five cars in their lifetimes"](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/)

Wait... that's not **that** much is it?

The simplicity of this number stuck with me and that's what I always had in mind when talking about the carbon cost of generative AI.

Now that we're in 2024 and the world is going through an AI craze, how many cars does it take ?

# Llama 3.1 and its paper

The 23rd July, Meta released its new family of models in the Llama family. This new version, LLama3.1 declines in three versions of different sizes and capabilities: 8B, 70B and 405B.  

This new model arrived by storm in the LLM community, and for good reasons: it's the best open source model to date, rivalizing with ChatGPT at the time of release.  

What's particulary interesting is the 90 pages paper released along side the model that go into great technical details on the training, alignement and testing process, enough so that we can start to derive some number from there.

Let's take a closer look and try to figure out how many cars it takes to train Llama 3.1.

# Meta steals your data but is open about research

The metaverse was a flop.

When Meta finally realized this they leaned heavily into AI and started producing open source models to compete against OpenAI. Their strategy is a bit different though and they prefer focusing on applicaitons and allow anyone to use their models, even for commmercial uses.

Meta commitment to open source allow us to have some insight of what it take to train this beast of a model.

TODO Illustration of soggoth

# What we learn from the paper

The first two families of Llama models were trained on Meta's supercluster built in ? //TODO date

https://ai.meta.com/blog/ai-rsc/

LLama 1 and 2 were trained on the supercluster

But this "superscluster" is too small now
https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/

"Llama 3 405B is trained on up to 16K H100 GPUs, each running at 700W TDP with 80GB HBM3,
using Meta’s Grand Teton AI server platform (Matt Bowman, 2022). Each server is equipped with eight GPUs
and two CPUs"

On when training is resuming after synchronisation of the GPUs:
"When this happens, it can result in instant fluctuations of power
consumption across the data center on the order of tens of megawatts, stretching the limits of the power grid.
This is an ongoing challenge for us as we scale training for future, even larger Llama models."

This table was stolen from ???
| Model          | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) |
|----------------|---------------------------|--------------------------------|----------------------------------------------------------------|
| Llama 3.1 8B   | 1.46M                      | 700                            | 420                                                            |
| Llama 3.1 70B  | 7.0M                       | 700                            | 2,040                                                          |
| Llama 3.1 405B | 30.84M                     | 700                            | 8,930                                                          |
| **Total**      | **39.3M**                  | **700**                        | **11,390**                                                     |

This is without counting the embodied CO2, i.e the cost of manufacturing the data center where this trainin has taken place.
Let's try to take this into account.

[150 kg of CO2eq / GPU ](https://arxiv.org/pdf/2211.02001)

TODO

# Failures of GPUs

# Inference costs
We did not even start accounting for inference costs

# Other GAFAMs
https://www.usinenouvelle.com/article/le-succes-de-l-intelligence-artificielle-fait-bondir-l-empreinte-carbone-des-acteurs-du-secteur.N2213817

Google missing it's goals because of AI carbon costs

# So, what do we do now?

Open Sourcing models to avoid retraining ?
Limiting inference time by not going straight to an LLM
Use smaller models when possible

# Future perspectives

Are data center getting better ?
AI Gen getting more and more popular
Big tech is focused on building a bigger and better model, and there is no going back
Do we even need it ? => We have yet to explore what we can do with our current models
Every company trains it own model, and most are murican
10 google research = 1 chatgpt call 

# Sources
https://nathanbaileyw.medium.com/the-carbon-footprint-of-llms-a-disaster-in-waiting-6fc666235cd0
https://www.usinenouvelle.com/article/le-succes-de-l-intelligence-artificielle-fait-bondir-l-empreinte-carbone-des-acteurs-du-secteur.N2213817
https://promptengineering.org/llm-open-source-vs-open-weights-vs-restricted-weights/
https://medium.com/gptalk/is-llama-3-1-really-open-source-73ac220f9aa2