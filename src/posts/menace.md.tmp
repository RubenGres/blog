---
title: "Are you smarter than 304 matchboxes?"
date: 2025-04-29
thumb: "menace.jpg"
tags:
    - AI
    - teaching
---

MENACE, or Matchbox Educable Noughts and Crosses Engine, is a mechanical AI built from 304 matchboxes capable of learning to play tic-tac-toe. The first version was designed in 1961 by Donal Michie.

![xxlarge](/blog/assets/img/menace_original.jpg)

The origin of MENACE came from a bet with a colleague who told him it was impossible to create a machine that could learn to play. At the time, artificial intelligence systems were very uncommon, but Michie was convinced that it was possible to create such a machine.

At the time of his conception, computers were hard to come by, so Michie found an original alternative by building his machine out of matchboxes. Each matchbox corresponds to a possible configuration of the board. When the computer plays for the first time, it chooses its moves randomly according to the state of the board. As it plays more games, thanks to a system of reinforcement learning, it makes losing strategies less likely and winning ones more likely. After around 150 games, the AI is unbeatable.

# How to play against MENACE?

Each game configuration, where it's MENACE's turn to play, is drawn on a matchbox. Inside each box are colored beads representing the different positions on the board. To limit the number of matchboxes, MENACE always plays first.
To play MENACE, the operator must find the matchbox corresponding to the current board. (Note that symmetrical configurations have only one box, so you need to imagine the possible rotations and update the board to match the box configuration). Once the box has been found, he removes a pearl at random. MENACE then plays the move corresponding to the color of the pearl drawn. The box and the pearl drawn are kept aside for the training phase.
Once MENACE has played, it's the human's turn to choose his move. The operation is repeated until the game ends.

![xxxlarge](/blog/assets/img/menace_diagram.jpg)

Once the game is over, there are three possible scenarios:
- If MENACE has lost: the beads drawn at random are removed from their respective boxes. She will then be less likely to choose these colors the next time the boxes are opened.
- If MENACE has won: we add 3 beads of the color drawn to each box, to strongly encourage her to make these choices again in the future.
- If the game is a tie: 1 bead of the color drawn is added to each box.
After a few games, some boxes may be empty. If an empty box is found, MENACE gives up. It's possible that the first box empties quickly, so we'll have to reset the first boxes with more beads to give him more time to learn.
Before training, 4 pearls of each color are placed in the first box. This is followed by three beads in the third-shot box, two in the fifth-shot box and a single bead in the last-shot box. This makes it possible to reward or punish the last moves more strongly, as they have a greater effect on the outcome of the game.

# The reinforcement learning algorithm

MENACE is one of the first implementations of the reinforcement learning algorithm. These algorithms from the machine learning family enable AIs to learn to master complex systems without the need for real games as training data.
For example, if two versions of MENACE were played against each other, they would both end up playing perfectly, even though neither of them knows the rules of tic-tac-toe.
In 2016, a version of this algorithm was used to create the AlphaGO AI, which beat the world Go champion for the first time. Today one of the most popular applications of reinforcement learning is RLHF, which aligns language models like ChatGPT with human expectations based on user feedback.

# Making my own version

I created this version of MENACE for the Plan√®te Science Occitanie scientific mediation association. I first computed the 304 possible configurations using a simple python script, accounting for all possibles symetries and impossible moves. I also conceived the design of this version and built it in on October 2024. To facilitate handling and find the right box, a web interface is available at rubengr.es/menace.
It is also possible to view statistics on the current game. The aim is to use this version in workshops, and to train it with the public over time.

Here is what the final box looks like:
![xxxlarge](/blog/assets/img/menace_rgres.png)

All the code and files used for this version are available at github.com/RubenGres/menace

# About Donal Michie

Donald Michie (1923 - 2007) was a British researcher best known for working alongside Alan Turing at Bletchley Park to decipher encrypted messages sent by the Nazis during the Second World War. He was Director of the Department of Machine Intelligence and Perception at Edinburgh University before founding the Turing Institute, with which he worked with NASA on the space shuttle's self-landerer. He remained active in the world of research until his 80th birthday.
When he first played MENACE in 1961, he played 220 games over 16 hours in one weekend. After just 20 games, MENACE was regularly drawing games.
